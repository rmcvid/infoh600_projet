{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a473d9",
   "metadata": {},
   "source": [
    "On va essayer de clenner les fichier par fichier et voir si y'a moyen d'améliorer ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import dask.dataframe as dd\n",
    "import re\n",
    "import heapq\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32a32bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_folder = os.path.join(sys.path[0], \"data_parquet\")\n",
    "parquet_folder_tracks = os.path.join(sys.path[0], \"data_tracks_parquet\")\n",
    "\n",
    "all_files = glob.glob(os.path.join(parquet_folder, \"*.parquet\"))\n",
    "all_track_files = glob.glob(os.path.join(parquet_folder_tracks, \"*.parquet\"))\n",
    "\n",
    "# Trier les fichiers par le numéro de début de slice\n",
    "def sort_key(f):\n",
    "    m = re.search(r\"mpd\\.slice\\.(\\d+)-\\d+\\.parquet\", f)\n",
    "    return int(m.group(1)) if m else float('inf')\n",
    "\n",
    "all_track_files = sorted(all_track_files, key=sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc5a07",
   "metadata": {},
   "source": [
    "On fait un groupage intra fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b32d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files\n",
      "Processed 100 files\n",
      "Processed 200 files\n",
      "Processed 300 files\n",
      "Processed 400 files\n",
      "Processed 500 files\n",
      "Processed 600 files\n",
      "Processed 700 files\n",
      "Processed 800 files\n",
      "Processed 900 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, file in enumerate(all_track_files):\n",
    "    df = pd.read_parquet(file)\n",
    "    # groupby local uniquement dans ce fichier\n",
    "    grouped = (\n",
    "            df.groupby(\"track_id\").agg({\n",
    "            \"pid\": list,             # fusionne la liste de pid\n",
    "            \"track_name\": \"first\",   # garde la première occurrence\n",
    "            \"artist_name\": \"first\"   # idem\n",
    "        }).reset_index()\n",
    "    )\n",
    "    output_file = f\"tracks_grouped/grouped_{i:04d}.parquet\"\n",
    "    grouped.to_parquet(output_file, index=False)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Processed {i+1} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf7306",
   "metadata": {},
   "source": [
    "On va trier les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89eb5c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 chunks créés dans temp_chunks\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "\n",
    "INPUT_DIR = \"tracks_grouped\"\n",
    "TEMP_DIR = \"temp_chunks\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 100_000  # nombre de lignes à traiter en mémoire par chunk\n",
    "\n",
    "chunk_id = 0\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if not filename.endswith(\".parquet\"):\n",
    "        continue\n",
    "    table = pq.read_table(os.path.join(INPUT_DIR, filename))\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    # Diviser en chunks si fichier très gros\n",
    "    for start in range(0, len(df), CHUNK_SIZE):\n",
    "        chunk = df.iloc[start:start + CHUNK_SIZE]\n",
    "        # Tri en mémoire par track_id\n",
    "        chunk = chunk.sort_values(\"track_id\")\n",
    "        # Sauvegarder chunk trié\n",
    "        chunk_file = os.path.join(TEMP_DIR, f\"chunk_{chunk_id:04d}.parquet\")\n",
    "        chunk.to_parquet(chunk_file, index=False)\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"{chunk_id} chunks créés dans {TEMP_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitionnement terminé dans track_vectors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "TEMP_DIR = \"temp_chunks\"\n",
    "OUTPUT_DIR = \"track_vectors\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "LINES_PER_FILE = 15_000  # lignes par fichier final\n",
    "BUFFER_SIZE = 15_000       # écrire par lot pour performance\n",
    "\n",
    "def parquet_row_generator(filename):\n",
    "    for batch in pq.read_table(filename).to_batches():\n",
    "        df = batch.to_pandas()\n",
    "        for _, row in df.iterrows():\n",
    "            yield row\n",
    "\n",
    "chunk_files = [os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR)]\n",
    "generators = [parquet_row_generator(f) for f in chunk_files]\n",
    "merged_iter = heapq.merge(*generators, key=lambda r: r[\"track_id\"])\n",
    "\n",
    "file_idx = 0\n",
    "lines_in_file = 0\n",
    "buffer = []\n",
    "\n",
    "current_id = None\n",
    "current_list = []\n",
    "\n",
    "for row in merged_iter:\n",
    "    tid = row[\"track_id\"]\n",
    "    lst = row[\"pid\"] \n",
    "    art = row[\"artist_name\"]\n",
    "    name = row[\"track_name\"] \n",
    "    if isinstance(lst, np.ndarray):\n",
    "        lst = lst.tolist()\n",
    "\n",
    "    if current_id is None:\n",
    "        current_id = tid\n",
    "        current_list = lst\n",
    "        current_artist = art\n",
    "        current_name = name\n",
    "    elif tid == current_id:\n",
    "        current_list.extend(lst)\n",
    "    else:\n",
    "        buffer.append({\"track_id\": current_id, \"artist_name\": current_artist, \"track_name\": current_name, \"pid\": current_list})\n",
    "        lines_in_file += 1\n",
    "\n",
    "        current_id = tid\n",
    "        current_list = lst\n",
    "        current_artist = art\n",
    "        current_name = name\n",
    "    if len(buffer) >= BUFFER_SIZE:\n",
    "        file_path = os.path.join(OUTPUT_DIR, f\"track_vector_{file_idx:04d}.parquet\")\n",
    "        if os.path.exists(file_path):\n",
    "            # si le fichier existe déjà, créer un nouveau fichier\n",
    "            file_idx += 1\n",
    "            lines_in_file = 0\n",
    "            file_path = os.path.join(OUTPUT_DIR, f\"track_vector_{file_idx:04d}.parquet\")\n",
    "        table = pa.Table.from_pandas(pd.DataFrame(buffer))\n",
    "        pq.write_table(table, file_path)\n",
    "        buffer = []\n",
    "\n",
    "    # changer de fichier si LINES_PER_FILE atteint\n",
    "    if lines_in_file >= LINES_PER_FILE:\n",
    "        file_idx += 1\n",
    "        lines_in_file = 0\n",
    "\n",
    "# écrire la dernière ligne\n",
    "if current_id is not None:\n",
    "    buffer.append({\"track_id\": current_id, \"artist_name\": current_artist, \"track_name\": current_name, \"pid\": current_list})\n",
    "\n",
    "# écrire ce qui reste\n",
    "if buffer:\n",
    "    file_path = os.path.join(OUTPUT_DIR, f\"track_vector_{file_idx:04d}.parquet\")\n",
    "    table = pa.Table.from_pandas(pd.DataFrame(buffer))\n",
    "    pq.write_table(table, file_path)\n",
    "\n",
    "print(f\"Partitionnement terminé dans {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e7cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_folder_tracks = os.path.join(sys.path[0], \"track_vectors\")\n",
    "df_tracks_vectors = dd.read_parquet(parquet_folder_tracks)\n",
    "df_tracks_vectors = df_tracks_vectors.set_index(\"track_id\", sorted= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33448010",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_tracks_vectors.loc[[\"7zzpwV2lgKsLke68yFoZdp\",\"7zzmpRP0WkYge45l6LTQ8i\"],\"pid\"].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9de543",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = len(set(res.iloc[0]) & set(res.iloc[1])) / (len(res.iloc[0])**0.5 * len(res.iloc[1])**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5041f4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aea02e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction évalue la similarité entre deux pistes en fonction des playlists dans lesquelles elles apparaissent.\n",
    "# De plus on peut lui fournir une liste de pistes ce qui permet immédiatement de résoudre la question 4\n",
    "def similar_tracks(track_id1, track_id2):\n",
    "    if isinstance(track_id1, list):\n",
    "        try:\n",
    "            subset1 = df_tracks_vectors.loc[track_id1, \"pid\"].compute()\n",
    "            subset2 = df_tracks_vectors.loc[track_id2, \"pid\"].compute()\n",
    "            list1 = subset1.explode().tolist()\n",
    "            list2 = subset2.explode().tolist()\n",
    "        except KeyError:\n",
    "            print(\"Le numéro est out of range\")\n",
    "            return None\n",
    "    else:\n",
    "        try:\n",
    "            # Récupérer les colonnes \"list_pid\"\n",
    "            subset = df_tracks_vectors.loc[[track_id1, track_id2], \"pid\"].compute()\n",
    "            list1,list2 = subset.iloc[0], subset.iloc[1]\n",
    "        except KeyError:\n",
    "            print(\"string\")\n",
    "            return None\n",
    "    # transformer en sets pour intersection\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    n_common = len(set1 & set2)\n",
    "    \n",
    "    if n_common == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return n_common / math.sqrt(len(set1) * len(set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255f7325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_tracks(\"7zzpwV2lgKsLke68yFoZdp\",\"7zzpwV2lgKsLke68yFoZdp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c169cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_tracks(\"7zzpwV2lgKsLke68yFoZdp\",\"08hlaHY7ssHzr2MvBKyKvt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4f63f",
   "metadata": {},
   "source": [
    "Faire une fonction qui prend en entrée le numéro de playlist et ressort la liste de toute les track_id compris dedans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df50630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_ids_for_pid(n: int):\n",
    "    if not (0 <= n <= 10**6):\n",
    "        raise ValueError(\"n doit être entre 0 et 1_000_000 inclus.\")\n",
    "    # Calcul du range\n",
    "    x = (n // 1000) * 1000\n",
    "    y = x + 999\n",
    "    filename = f\"mpd.slice.{x}-{y}.parquet\"\n",
    "    # Lecture du fichier parquet\n",
    "    filepath = os.path.join(sys.path[0], \"data_tracks_parquet\", filename)\n",
    "    df = pd.read_parquet(filepath)\n",
    "\n",
    "    # Filtrer les lignes où pid == n\n",
    "    filtered = df[df[\"pid\"] == n]\n",
    "\n",
    "    # Retourner la colonne track_id comme liste\n",
    "    return filtered[\"track_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f2b57c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05064817454851596"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_tracks(get_track_ids_for_pid(987000),get_track_ids_for_pid(50000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
