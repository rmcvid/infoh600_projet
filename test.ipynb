{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a473d9",
   "metadata": {},
   "source": [
    "On va essayer de clenner les fichier par fichier et voir si y'a moyen d'améliorer ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import dask.dataframe as dd\n",
    "import re\n",
    "import heapq\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pyarrow.parquet as pq\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32a32bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_folder = os.path.join(sys.path[0], \"data_parquet\")\n",
    "parquet_folder_tracks = os.path.join(sys.path[0], \"data_tracks_parquet\")\n",
    "\n",
    "all_files = glob.glob(os.path.join(parquet_folder, \"*.parquet\"))\n",
    "all_track_files = glob.glob(os.path.join(parquet_folder_tracks, \"*.parquet\"))\n",
    "\n",
    "# Trier les fichiers par le numéro de début de slice\n",
    "def sort_key(f):\n",
    "    m = re.search(r\"mpd\\.slice\\.(\\d+)-\\d+\\.parquet\", f)\n",
    "    return int(m.group(1)) if m else float('inf')\n",
    "\n",
    "all_track_files = sorted(all_track_files, key=sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc5a07",
   "metadata": {},
   "source": [
    "On fait un groupage intra fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b32d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 files\n",
      "Processed 100 files\n",
      "Processed 200 files\n",
      "Processed 300 files\n",
      "Processed 400 files\n",
      "Processed 500 files\n",
      "Processed 600 files\n",
      "Processed 700 files\n",
      "Processed 800 files\n",
      "Processed 900 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, file in enumerate(all_track_files):\n",
    "    df = pd.read_parquet(file)\n",
    "    # groupby local uniquement dans ce fichier\n",
    "    grouped = (\n",
    "            df.groupby(\"track_id\").agg({\n",
    "            \"pid\": list,             # fusionne la liste de pid\n",
    "            \"track_name\": \"first\",   # garde la première occurrence\n",
    "            \"artist_name\": \"first\"   # idem\n",
    "        }).reset_index()\n",
    "    )\n",
    "    output_file = f\"tracks_grouped/grouped_{i:04d}.parquet\"\n",
    "    grouped.to_parquet(output_file, index=False)\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Processed {i+1} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf7306",
   "metadata": {},
   "source": [
    "On va trier les musique intra fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89eb5c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 chunks créés dans temp_chunks\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import os\n",
    "\n",
    "INPUT_DIR = \"tracks_grouped\"\n",
    "TEMP_DIR = \"temp_chunks\"\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = 100_000  # nombre de lignes à traiter en mémoire par chunk\n",
    "\n",
    "chunk_id = 0\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if not filename.endswith(\".parquet\"):\n",
    "        continue\n",
    "    table = pq.read_table(os.path.join(INPUT_DIR, filename))\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    # Diviser en chunks si fichier très gros\n",
    "    for start in range(0, len(df), CHUNK_SIZE):\n",
    "        chunk = df.iloc[start:start + CHUNK_SIZE]\n",
    "        # Tri en mémoire par track_id\n",
    "        chunk = chunk.sort_values(\"track_id\")\n",
    "        # Sauvegarder chunk trié\n",
    "        chunk_file = os.path.join(TEMP_DIR, f\"chunk_{chunk_id:04d}.parquet\")\n",
    "        chunk.to_parquet(chunk_file, index=False)\n",
    "        chunk_id += 1\n",
    "\n",
    "print(f\"{chunk_id} chunks créés dans {TEMP_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70642a",
   "metadata": {},
   "source": [
    "On trie les musiques interfichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitionnement terminé dans track_vectors\n"
     ]
    }
   ],
   "source": [
    "TEMP_DIR = \"temp_chunks\"\n",
    "OUTPUT_DIR = \"track_vectors\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "LINES_PER_FILE = 15_000  # lignes par fichier final\n",
    "BUFFER_SIZE = 15_000       # écrire par lot pour performance\n",
    "\n",
    "def parquet_row_generator(filename):\n",
    "    for batch in pq.read_table(filename).to_batches():\n",
    "        df = batch.to_pandas()\n",
    "        for _, row in df.iterrows():\n",
    "            yield row\n",
    "\n",
    "chunk_files = [os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR)]\n",
    "generators = [parquet_row_generator(f) for f in chunk_files]\n",
    "merged_iter = heapq.merge(*generators, key=lambda r: r[\"track_id\"])\n",
    "\n",
    "file_idx = 0\n",
    "lines_in_file = 0\n",
    "buffer = []\n",
    "\n",
    "current_id = None\n",
    "current_list = []\n",
    "\n",
    "for row in merged_iter:\n",
    "    tid = row[\"track_id\"]\n",
    "    lst = row[\"pid\"] \n",
    "    art = row[\"artist_name\"]\n",
    "    name = row[\"track_name\"] \n",
    "    if isinstance(lst, np.ndarray):\n",
    "        lst = lst.tolist()\n",
    "\n",
    "    if current_id is None:\n",
    "        current_id = tid\n",
    "        current_list = lst\n",
    "        current_artist = art\n",
    "        current_name = name\n",
    "    elif tid == current_id:\n",
    "        current_list.extend(lst)\n",
    "    else:\n",
    "        buffer.append({\"track_id\": current_id, \"artist_name\": current_artist, \"track_name\": current_name, \"pid\": current_list})\n",
    "        lines_in_file += 1\n",
    "\n",
    "        current_id = tid\n",
    "        current_list = lst\n",
    "        current_artist = art\n",
    "        current_name = name\n",
    "    if len(buffer) >= BUFFER_SIZE:\n",
    "        file_path = os.path.join(OUTPUT_DIR, f\"track_vector_{file_idx:04d}.parquet\")\n",
    "        if os.path.exists(file_path):\n",
    "            # si le fichier existe déjà, créer un nouveau fichier\n",
    "            file_idx += 1\n",
    "            lines_in_file = 0\n",
    "            file_path = os.path.join(OUTPUT_DIR, f\"track_vector_{file_idx:04d}.parquet\")\n",
    "        table = pa.Table.from_pandas(pd.DataFrame(buffer))\n",
    "        pq.write_table(table, file_path)\n",
    "        buffer = []\n",
    "\n",
    "    # changer de fichier si LINES_PER_FILE atteint\n",
    "    if lines_in_file >= LINES_PER_FILE:\n",
    "        file_idx += 1\n",
    "        lines_in_file = 0\n",
    "\n",
    "# écrire la dernière ligne\n",
    "if current_id is not None:\n",
    "    buffer.append({\"track_id\": current_id, \"artist_name\": current_artist, \"track_name\": current_name, \"pid\": current_list})\n",
    "\n",
    "# écrire ce qui reste\n",
    "if buffer:\n",
    "    file_path = os.path.join(OUTPUT_DIR, f\"track_vector_{file_idx:04d}.parquet\")\n",
    "    table = pa.Table.from_pandas(pd.DataFrame(buffer))\n",
    "    pq.write_table(table, file_path)\n",
    "\n",
    "print(f\"Partitionnement terminé dans {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19059d26",
   "metadata": {},
   "source": [
    "Suprimes les dossiers intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3b0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(sys.path[0], \"temp_chunks\")\n",
    "if os.path.exists(folder):\n",
    "    shutil.rmtree(folder)\n",
    "\n",
    "folder = os.path.join(sys.path[0], \"tracks_grouped\")\n",
    "if os.path.exists(folder):\n",
    "    shutil.rmtree(folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014585f",
   "metadata": {},
   "source": [
    "Charge les fichiers track_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e7cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_folder_tracks = os.path.join(sys.path[0], \"track_vectors\")\n",
    "df_tracks_vectors = dd.read_parquet(parquet_folder_tracks)\n",
    "df_tracks_vectors = df_tracks_vectors.set_index(\"track_id\", sorted= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291ba97",
   "metadata": {},
   "source": [
    "On construit une indexation ce qui permet d'utiliser panda pour aller chercher seulement la db dont on a besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5715c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index tous les fichiers track_vectors pour connaitre rapidement dans quel fichier chercher un track_id\n",
    "def build_index(folder):\n",
    "    index = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if not filename.endswith(\".parquet\"):\n",
    "            continue\n",
    "        \n",
    "        path = os.path.join(folder, filename)\n",
    "        \n",
    "        # lire uniquement le row_group 0 (hyper rapide)\n",
    "        table = pq.read_table(path, columns=[\"track_id\"], read_dictionary=[\"track_id\"])\n",
    "        col = table.column(\"track_id\")\n",
    "        \n",
    "        min_id = col[0].as_py()\n",
    "        max_id = col[-1].as_py()\n",
    "        index.append((min_id, max_id, filename))\n",
    "    \n",
    "    return index\n",
    "INDEX = build_index(os.path.join(sys.path[0],\"track_vectors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab5183",
   "metadata": {},
   "source": [
    "Permet d'utiliser panda et d'ouvrir le fichier dans lequel se trouve le track id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4575551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ryanm\\\\Documents\\\\master big data\\\\info\\\\projet\\\\track_vectors\\\\track_vector_0068.parquet'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_file_for_track_id(track_id):\n",
    "    low, high = 0, len(INDEX) - 1\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        min_id, max_id, filename = INDEX[mid]\n",
    "        \n",
    "        if track_id < min_id:\n",
    "            high = mid - 1\n",
    "        elif track_id > max_id:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            return os.path.join(sys.path[0], \"track_vectors\", filename)\n",
    "    return None\n",
    "\n",
    "find_file_for_track_id('3Y6ILqXqCBNwBTxVda8Z68')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12d6321",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1 = df_tracks_vectors.loc[['3Y6ILqXqCBNwBTxVda8Z68','3Y6ILqXqCBNwBTxVda8Z68'], \"pid\"].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b00617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_in_order(df, track_ids):\n",
    "    # un seul fetch Dask → compute()\n",
    "    unique_ids = list(dict.fromkeys(track_ids))\n",
    "    tmp = df.loc[unique_ids,'pid'].compute()\n",
    "\n",
    "    # reconstruction dans l'ordre demandé\n",
    "    rows = [tmp.loc[tid] for tid in track_ids]\n",
    "\n",
    "    return rows\n",
    "row = get_rows_in_order(df_tracks_vectors, ['3Y6ILqXqCBNwBTxVda8Z68','09bLMpxeUECNq4v5BIdgp6','3Y6ILqXqCBNwBTxVda8Z68'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899211b7",
   "metadata": {},
   "source": [
    "C'est la fonction principale elle permet de calculer les similarité entre musique ou playlist.\n",
    "L'argument which permet de sélectionner si on choisit de calculer toutes les méthodes.\n",
    "\n",
    "J'ai été assez surpris mais finalement c'est logique, que quand on utilise le produit avec la similarité du suprémum c'est long. La raison est qu'il faut faire une double boucle for sur la list pid de a et b. Cette liste peu potentiellement avoir une dizaine de millier d'items et donc la boucle peut qui en fait le carré peut être longue. c'est pas toujorus le cas mais c'est peu viable pour faire des stats descriptive.\n",
    "Je proposerai d'enlever celui là pour les stats descritpive et juste de l'utiliser sur un micro échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aea02e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction évalue la similarité entre deux pistes en fonction des playlists dans lesquelles elles apparaissent.\n",
    "# De plus on peut lui fournir une liste de pistes ce qui permet immédiatement de résoudre la question 4\n",
    "def similar_tracks(track_id1, track_id2, which = 0):\n",
    "    \"\"\"\" \n",
    "        track_id1 et track_id2 peuvent être des listes d'id ou des id simples\n",
    "        which : \n",
    "            0 : retourne le score principal de facons optimisées (panda)\n",
    "            1 : retourne le score principal et le score zoé calculés de façons optimisées (panda)\n",
    "            2 : retourne le score principal, le score zoé et le score en utilisant dask \n",
    "            autre : retourne le score principal, le score zoé et le score en utilisant le max des similarités entre chaque paire de pistes\n",
    "                    avec dask( computation plus lourde )\n",
    "    \"\"\"\n",
    "    if isinstance(track_id1, list):\n",
    "        try:\n",
    "            # version panda:\n",
    "            if(which == 1 or which == 0):\n",
    "                file_to_id1 = defaultdict(list)\n",
    "                file_to_id2 = defaultdict(list)\n",
    "\n",
    "                for tid in track_id1 :\n",
    "                    file_to_id1[find_file_for_track_id(tid)].append(tid)\n",
    "                for tid in track_id2 :\n",
    "                    file_to_id2[find_file_for_track_id(tid)].append(tid)\n",
    "\n",
    "                pid_cache = {}   # track_id -> pid list\n",
    "                list1,list2 = [], []\n",
    "                for file, ids in file_to_id1.items():\n",
    "                    df = pd.read_parquet(file,columns=[\"track_id\", \"pid\"]).set_index(\"track_id\")\n",
    "                    list1 +=df.loc[file_to_id1[file],\"pid\"].iloc[:].explode().tolist()\n",
    "                    list2 += df.loc[file_to_id2[file],\"pid\"].iloc[:].explode().tolist()\n",
    "                    del file_to_id2[file]\n",
    "                for file, ids in file_to_id2.items():\n",
    "                    df = pd.read_parquet(file,columns=[\"track_id\", \"pid\"]).set_index(\"track_id\")\n",
    "                    list2 += df.loc[file_to_id2[file],\"pid\"].iloc[:].explode().tolist()\n",
    "            else:\n",
    "                # version un peu moins naive avec un seul appel à dask\n",
    "                # on peut faire mieux en utilisant pandas pour reconstruire les listes dans l'ordre demandé\n",
    "                unique_ids = list(dict.fromkeys(track_id1+ track_id2))\n",
    "                tmp = df_tracks_vectors.loc[unique_ids,'pid'].compute()\n",
    "                # reconstruction dans l'ordre demandé\n",
    "                rows = [tmp.loc[tid] for tid in track_id1 + track_id2] \n",
    "                list1 = [x for sub in rows[0:len(track_id1)] for x in sub]\n",
    "                list2 = [x for sub in rows[len(track_id1):] for x in sub]     \n",
    "            #Version très naive\n",
    "            \"\"\"\n",
    "            #try:\n",
    "            #    list1 = df_tracks_vectors.loc[track_id1, \"pid\"].compute().explode().tolist()\n",
    "            #    list2 = df_tracks_vectors.loc[track_id2, \"pid\"].compute().explode().tolist()\n",
    "            \"\"\"\n",
    "        except KeyError:\n",
    "            print(\"Le numéro est out of range\")\n",
    "            return None\n",
    "            \n",
    "    else:\n",
    "        ## partie où on utilise un seul track_id. On fait le calcul avec panda\n",
    "        # on pourrait aussi optimiser si les deux musique sont dans le même fichier mais bon ca à a 1/150 chance d'arriver donc\n",
    "        # flemme d'optimiser pour si peu\n",
    "        try:\n",
    "            # Récupérer les colonnes \"list_pid\"\n",
    "            df = pd.read_parquet(find_file_for_track_id(track_id1),\n",
    "                     columns=[\"track_id\", \"pid\"])\n",
    "            list1 = df.loc[df.track_id == track_id1, \"pid\"].iloc[0]\n",
    "            df = pd.read_parquet(find_file_for_track_id(track_id2),\n",
    "                     columns=[\"track_id\", \"pid\"])\n",
    "            list2 = df.loc[df.track_id == track_id2, \"pid\"].iloc[0]\n",
    "            which = 0     # le calcul des autre ne prend du sens que si c'est une liste\n",
    "        except KeyError:\n",
    "            print(\"string\")\n",
    "            return None\n",
    "    # transformer en sets pour intersection\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    n_common = len(set1 & set2)\n",
    "    res_main = n_common / math.sqrt(len(set1) * len(set2))\n",
    "    res_zoe = len(set(track_id1) & set(track_id2)) / math.sqrt(len(track_id1) * len(track_id2))\n",
    "    if which == 0:\n",
    "        return res_main\n",
    "    if which == 1 or which == 2:\n",
    "        return res_main, res_zoe\n",
    "    else:\n",
    "        res_sup = 0.0\n",
    "        for l1 in rows[0:len(track_id1)]:\n",
    "            sim = 0\n",
    "            for l2 in rows[len(track_id1):]:\n",
    "                sim = max(sim, len(set(l1) & set(l2)) /math.sqrt(len(set(l1)) * len(set(l2)) ))\n",
    "                if sim == 1.0:\n",
    "                    break\n",
    "            res_sup = res_sup + sim\n",
    "        res_sup = res_sup / (len(track_id1)*len(track_id2))**0.5\n",
    "        return (res_main, res_zoe, res_sup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ddb06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8df50630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_ids_for_pid(n: int):\n",
    "    if not (0 <= n <= 10**6):\n",
    "        raise ValueError(\"n doit être entre 0 et 1_000_000 inclus.\")\n",
    "    # Calcul du range\n",
    "    x = (n // 1000) * 1000\n",
    "    y = x + 999\n",
    "    filename = f\"mpd.slice.{x}-{y}.parquet\"\n",
    "    # Lecture du fichier parquet\n",
    "    filepath = os.path.join(sys.path[0], \"data_tracks_parquet\", filename)\n",
    "    df = pd.read_parquet(filepath)\n",
    "\n",
    "    # Filtrer les lignes où pid == n\n",
    "    filtered = df[df[\"pid\"] == n]\n",
    "\n",
    "    # Retourner la colonne track_id comme liste\n",
    "    return filtered[\"track_id\"].tolist()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "722c276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1998485668390489, 0.0, 0.028675636963277677)\n",
      "(1.0, 1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(similar_tracks(get_track_ids_for_pid(78561),get_track_ids_for_pid(235921),2))\n",
    "print(similar_tracks(get_track_ids_for_pid(2000),get_track_ids_for_pid(2000),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ee1d6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_tracks('3Y6ILqXqCBNwBTxVda8Z68','3Y6ILqXqCBNwBTxVda8Z68')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f2b57c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1998485668390489\n"
     ]
    }
   ],
   "source": [
    "print(similar_tracks(get_track_ids_for_pid(78561),get_track_ids_for_pid(235921)))\n",
    "#print(similar_tracks(get_track_ids_for_pid(2000),get_track_ids_for_pid(2000)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967166e",
   "metadata": {},
   "source": [
    "## Comparaison des versions par rapport à dask ou panda\n",
    "Ce serait bien de faire une comparaison temporelle et de la joindre dans le rapport\n",
    "en lancant le code suivant ca vous donnera une idée de l'amélioration de l'une par rapport à l'autre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "119aa3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized time: 1.3993 seconds, result: (0.1998485668390489, 0.0)\n",
      "Non-optimized time: 2.6757 seconds, result: (0.1998485668390489, 0.0)\n",
      "All method time: 4.2497 seconds, result: (0.1998485668390489, 0.0, 0.028675636963277677)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "opt = similar_tracks(get_track_ids_for_pid(78561),get_track_ids_for_pid(235921),1)\n",
    "end_time = time.time()\n",
    "topt = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "nopt = similar_tracks(get_track_ids_for_pid(78561),get_track_ids_for_pid(235921),2)\n",
    "end_time = time.time()\n",
    "tnopt = end_time - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "all = similar_tracks(get_track_ids_for_pid(78561),get_track_ids_for_pid(235921),3)\n",
    "end_time = time.time()\n",
    "tall = end_time - start_time\n",
    "\n",
    "print(f\"Optimized time: {topt:.4f} seconds, result: {opt}\")\n",
    "print(f\"Non-optimized time: {tnopt:.4f} seconds, result: {nopt}\")\n",
    "print(f\"All method time: {tall:.4f} seconds, result: {all}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
