{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec964c8",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380bbf71",
   "metadata": {},
   "source": [
    "Dans ce Notebook, je reprends la méthode de résolution de la question 3 (voir main.ipynb) mais en utilisant `Spark` pour pouvoir gérer les 35Go de données.\n",
    "\n",
    "Pour vous résumer rapidement, Spark est un Pandas *amélioré* qui ne charge pas l'intégralité des données en mémoire (**= Lazy Evaluation**) et quand il doit effectuer des calculs il les fait en parallèles sur l'ensemble des processeurs de la machine (**= Parallelism**). ChatGPT m'a donné une bonne analogie que vous pouvez voir plus bas.\n",
    "\n",
    "cfr: \n",
    "* https://app.datacamp.com/learn/courses/big-data-fundamentals-with-pyspark (le prof nous donne l'accès dans un mail)\n",
    "* ChatGPT\n",
    "* Le cours dans deux semaines\n",
    "* **documentation**: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "**Note importante**: Pour pouvoir utiliser `Spark` sur votre machine vous pouvez suivre le tutoriel suivant: \n",
    "* https://medium.com/@marcelopedronidasilva/how-to-install-and-run-pyspark-locally-integrated-with-vscode-via-jupyter-notebook-on-windows-ff209ac8621f\n",
    "\n",
    "---\n",
    "\n",
    "## Analogie\n",
    "\n",
    "Imagine que tes données (35 Go) sont un Caddie géant rempli à ras bord de courses. Ta RAM, c'est le tapis roulant de la caisse.\n",
    "\n",
    "1. Pandas : La Méthode \"Tout ou Rien\"\n",
    "Pandas, c'est une caisse unique avec un caissier très strict.\n",
    "* Son exigence : \"Videz l'intégralité de votre caddie sur le tapis avant que je ne scanne le premier article.\"\n",
    "* Le problème : Ton caddie est énorme (35 Go). Le tapis est petit (ta RAM de 16 Go).\n",
    "* Le résultat : Tu essaies d'empiler tes packs d'eau et tes conserves. Le tapis craque, tout tombe par terre. C'est le Memory Error. Le travail s'arrête avant même d'avoir commencé.\n",
    "\n",
    "2. Spark : La Méthode \"Flux Tendu\"\n",
    "Spark approche ce caddie géant différemment, en combinant ses deux super-pouvoirs :\n",
    "\n",
    "**A. Le Parallélisme** (L'Armée de caisse)\n",
    "\n",
    "Au lieu d'aller à une seule caisse, Spark crie \"OUVERTURE DES CAISSES !\". Il divise ton caddie géant en 10 petits paniers et les envoie vers 10 caisses différentes en même temps.\n",
    "* Résultat : Le travail avance 10 fois plus vite.\n",
    "\n",
    "**B. Le Lazy Evaluation** (Le Flux)\n",
    "\n",
    "C'est ici que Spark gère les 35 Go sans faire craquer les tapis. Les caissiers Spark disent : \"Gardez vos articles dans le panier. Ne posez sur le tapis que ce que je peux scanner maintenant.\"\n",
    "* Tu poses une brique de lait sur le tapis.\n",
    "* Le caissier la scanne (le processeur calcule).\n",
    "* La brique est immédiatement mise en sac et évacuée (sauvegardée ou envoyée à l'étape suivante).\n",
    "* La place est libre pour poser le paquet de pâtes suivant.\n",
    "\n",
    "La magie du mélange : Grâce au Lazy Evaluation, le tapis roulant (la RAM) n'est jamais plein, même si tu traites des tonnes de courses. Grâce au Parallélisme, 10 tapis roulent en même temps pour finir le travail en un temps record.*\n",
    "\n",
    "--- \n",
    "# **Note importante**\n",
    "\n",
    "C'est un début de solution mais ce n'est pas fini, car j'ai retesté avec une autre musique plus populaire *35kahykNu00FPysz3C2euR* et chez moi il crash. Il faut changer la configuration de la Session Spark et alors ça fonctionne mais le code prend 20 minutes pour run (pas idéal s'il faut une démo live (?)).\n",
    "\n",
    "Je pense qu'on devrait **changer la méthode** à la question et passer au algorithme de Collaborative filtering directement proposés par Spark. Voir doc:\n",
    "* https://spark.apache.org/docs/latest/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ffb6ef",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9d3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, explode, collect_set, size, regexp_replace, count, lit, sqrt, desc, broadcast, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73153ed7",
   "metadata": {},
   "source": [
    "# Extraction des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068161be",
   "metadata": {},
   "source": [
    "Spark ne sait pas lire efficacement des fichiers zip, on va alors dézipper l'entériété du fichier et le mettre dans un fichier non zippé.\n",
    "* L'étape ici n'est pas top, on peut imaginer de passer à Parquet ou une autre solution qui nous permet de stocker plsu efficacement les 35 Go ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8ce039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_FILE_PATH = \"spotify_million_playlist_dataset.zip\" \n",
    "EXTRACT_PATH = \"data_extracted/\"                       \n",
    "\n",
    "def unzip_data(zip_path, extract_to):\n",
    "    \"\"\"Extrait le dataset si ce n'est pas déjà fait.\"\"\"\n",
    "    if not os.path.exists(extract_to):\n",
    "        print(f\"Extraction de {zip_path}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(\"Extraction terminée.\")\n",
    "    else:\n",
    "        print(f\"Le dossier {extract_to} existe déjà. On passe l'extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cabe8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si déjà dézipper, ne pas exécuter\n",
    "#unzip_data(ZIP_FILE_PATH, EXTRACT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "920e893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chemin pour l'ensemble des fichiers JSON (si on a dézippé avec la fonction au-dessus)\n",
    "JSON_DATA_PATH = os.path.join(EXTRACT_PATH, \"data/*.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2748b63",
   "metadata": {},
   "source": [
    "# Initialisation de Spark en mode local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8727debe",
   "metadata": {},
   "source": [
    "On démarre une \"*Spark Session*\" afin de pouvoir utiliser Spark sur notre machine:\n",
    "* `master(\"local[*]\")` : L'étoile `*` dit à Spark : \"Utilise tous les cœurs de processeur disponibles sur mon ordinateur portable\". S'il y a *x* cœurs, Spark lancera $x$ tâches en parallèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07ba79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SpotifyPlaylistRecommenderLSH\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5916e4",
   "metadata": {},
   "source": [
    "Pour voir la Session Spark que l'on a créée **localement** : http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169910cf",
   "metadata": {},
   "source": [
    "Le reste du code, je reprend ce qui a été fait à la question 3 mais adapté à Spark, reste à voir ce qu'on peut faire d'autre pour optimiser notre gestion des 35Go (surtout si on doit faire une démo live (voir dernière cellule)).\n",
    "* Il y a la piste de `parquet` mais on compresse juste nos données, pas sûr que ce soit pertinent mais ça reste pratique\n",
    "* On peut aussi seulement garder les playlist avec plus de $x$ musiques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b26ff87",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08736e4b",
   "metadata": {},
   "source": [
    "Schema is the structure of data in DataFrame and helps Spark to optimize queries on the data more efficiently. A schema provides informational detail such as the column name, the type of data in that column, and whether null or empty values are allowed in the column (cfr DataCamp).\n",
    "* Un fichier JSON (`dataset_schema`) contient une liste appelée *playlists*.\n",
    "* Chaque élément de cette liste (`playlist_schema`) contient un entier *pid* et une liste *tracks*.\n",
    "* Chaque élément de la liste tracks (`track_schema`) contient trois strings: *track_uri*, *artist_name* et *track_name*.\n",
    "\n",
    "Tout ce qui n'est pas décrit ici (par exemple num_followers ou description de la playlist) sera ignoré par Spark lors de la lecture. C'est ce qui rend le chargement beaucoup plus rapide (on passe de 1m30 (sans schema) à 5 secondes (avec))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b25a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema des JSON \n",
    "track_schema = StructType([\n",
    "    StructField(\"track_uri\", StringType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"track_name\", StringType(), True)\n",
    "])\n",
    "playlist_schema = StructType([\n",
    "    StructField(\"pid\", IntegerType(), True),\n",
    "    StructField(\"tracks\", ArrayType(track_schema), True)\n",
    "])\n",
    "dataset_schema = StructType([\n",
    "    StructField(\"playlists\", ArrayType(playlist_schema), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea74cfe",
   "metadata": {},
   "source": [
    "`data` est une Data Frame, au sens Spark du terme (assez similaire à Pandas) qui contient:\n",
    "* **lignes**: le nombre de fichiers JSON chargés (dataset complet=$1.000$)\n",
    "* **colonne**: unique contenant une liste de $1000$ playlists par fichiers JSON\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: normalement `Spark` doit pouvoir trouver les fichiers lui-même mais il semblerait que l'on doive télécharger un autre fichier pour ça et honnêtement ça me cassait les couilles. C'est pourquoi j'utilise `glob` avec qui Python va scanner le dossier `data` et transformer cette chaîne de caractères en une liste Python contenant les chemins complets.\n",
    "\n",
    "* Résultat : `['data/mpd.slice.0-999.json', 'data/mpd.slice.1000-1999.json', ...]`\n",
    "\n",
    "cfr: https://docs.python.org/3/library/glob.html\n",
    "\n",
    "---\n",
    "\n",
    "Si vous voulez voir le résultat du code à chaque étape vous pouvez utiliser la méthode `.show()` mais je vous déconseille de le faire sur l'ensemble des $1.000.000$ de playlists car ça rajoute un temps de calcule considérable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e792a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers trouvés par Python : 1000\n"
     ]
    }
   ],
   "source": [
    "# On utilise glob pour trouver les fichiers à la place de Spark\n",
    "JSON_DATA_PATH = r\"data_extracted\\data\\mpd.slice.*.json\"\n",
    "json_files = glob.glob(JSON_DATA_PATH)\n",
    "\n",
    "print(f\"Fichiers trouvés par Python : {len(json_files)}\")\n",
    "\n",
    "if not json_files:\n",
    "    raise Exception(f\"Aucun fichier trouvé dans : {JSON_DATA_PATH}\")\n",
    "\n",
    "#On passe la LISTE des fichiers directement à Spark\n",
    "#Spark va lire la liste comme si vous aviez tapé les noms un par un\n",
    "data = spark.read \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .schema(dataset_schema) \\\n",
    "    .json(json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b585afe1",
   "metadata": {},
   "source": [
    "# 3. Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccceb5",
   "metadata": {},
   "source": [
    "Et tant donné que `data` contient un ligne par JSON et qu'on veut une ligne par playlist on utilise:\n",
    "* `.explode(col(playlists))` prend la colonne `playlist` (avec `col`) de `data` et crée une nouvelle ligne pour chaque playlist ($=x$ $fichiers$ $de$ $1000$ $playlists$). Si un fichier contient $1000$ playlists, on passe de $1$ ligne à $1000$ lignes par fichiers.\n",
    "* `.select()` on ne garde que la colonne sélectionnée, toute les autres son supprimées\n",
    "* `.alias()` on renomme la colonne au singulier pour que ce soit plus logique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d24455",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists = data.select(explode(col(\"playlists\")).alias(\"playlist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bcba9",
   "metadata": {},
   "source": [
    "Ensuite, comme discuté dans `main.ipynb`, on ne garde que deux colonnes. `Spark` va dans le JSON et ne garde que:\n",
    "* `pid`: les playlists uniques\n",
    "* `tracks`: la **liste** des musiques par playlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2baa2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_track = playlists.select(\n",
    "    col(\"playlist.pid\").alias(\"pid\"),\n",
    "    col(\"playlist.tracks.track_uri\").alias(\"track_uris\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405a8b5",
   "metadata": {},
   "source": [
    "On crée une Data Frame temporaire contenant une ligne par musique au lieu d'une ligne par playlist.\n",
    "* `pid`: qui peut avoir des doublons\n",
    "* `raw_track_uri`: qui peut avoir des doublons\n",
    "\n",
    "Puis, on construit une df des tracks cleaned dans le sens ou les id sont débarrasé de *spotify:name:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690c81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_temp_df = df_track.select(\n",
    "    col(\"pid\"),\n",
    "    explode(col(\"track_uris\")).alias(\"raw_track_uri\")\n",
    ")\n",
    "\n",
    "#Cleaner les track_id\n",
    "cleaned_df_track = exploded_temp_df.select(\n",
    "    col(\"pid\"),\n",
    "    regexp_replace(col(\"raw_track_uri\"), \"spotify:track:\", \"\").alias(\"track_id\")\n",
    ")\n",
    "\n",
    "#cleaned_df_track.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace06b3e",
   "metadata": {},
   "source": [
    "A priori, si $\\text{track\\_id}_i = \\text{track\\_id}_j$ :  \n",
    "\n",
    "$$\n",
    "\\text{artist\\_id}_i = \\text{artist\\_id}_j, \\quad\n",
    "\\text{album\\_id}_i = \\text{album\\_id}_j\n",
    "$$\n",
    "\n",
    "On peut donc chercher par `track_id` unique et créer une liste pour le `pid`. Ainsi, *list_pid* sera un vecteur de dimension $( \\text{ndim} = 10 000$. Seules les composantes non nulles sont conservées et valent 0 ou 1 si elles sont présentes dans la playlist.  \n",
    "Plutôt que de conserver les 0, on conserve uniquement les indices des playlists dans lesquelles la musique apparaît.  \n",
    "(On pourrait éventuellement retirer les playlists avec zéro track.)\n",
    "\n",
    "La similarité entre deux musiques peut alors se calculer de la façon suivante :  \n",
    "\n",
    "$$\n",
    "\\langle a, b \\rangle = \\frac{|a \\cap b|}{\\sqrt{\\text{len}(a) \\cdot \\text{len}(b)}} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b39559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'collect_set' au lieu de 'collect_list' pour éviter les doublons \n",
    "inverted_index_df = cleaned_df_track.groupBy(\"track_id\").agg(\n",
    "    collect_set(\"pid\").alias(\"playlists_list\")\n",
    ")\n",
    "\n",
    "#ajouts de la troisième colonne avec la taille de la liste de playlist\n",
    "df_tracks_vectors = inverted_index_df.withColumn(\n",
    "    \"pid_count\", \n",
    "    size(col(\"playlists_list\"))\n",
    ")\n",
    "\n",
    "#afficher les résultats (seulement sur un petit jeu de données sinon augmente considérablement le temps)\n",
    "#df_tracks_vectors.printSchema()\n",
    "#df_tracks_vectors.count()\n",
    "#df_tracks_vectors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d89e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_tracks(target_track_id, cleaned_df_track, df_tracks_vectors, limit=10):\n",
    "\n",
    "    #On va récupérer la valeur de la longueur du vecteur dans df_tracks_vectors\n",
    "    target_count_row = df_tracks_vectors.filter(col(\"track_id\") == target_track_id).select(\"pid_count\").first()\n",
    "    if not target_count_row:\n",
    "        print(f\"Track {target_track_id} introuvable\")\n",
    "        return None\n",
    "\n",
    "    target_global_count = target_count_row[\"pid_count\"]\n",
    "\n",
    "    #On s'assure qu'on ne liste pas deux fois la même playlist pour la même chanson\n",
    "    target_pids_df = cleaned_df_track.filter(col(\"track_id\") == target_track_id) \\\n",
    "                                     .select(\"pid\") \\\n",
    "                                     .distinct()\n",
    "\n",
    "\n",
    "    #On cherche l'intersection entre A et B\n",
    "    candidates_df = cleaned_df_track.join(broadcast(target_pids_df), on=\"pid\", how=\"inner\")\n",
    "    intersection_df = candidates_df.groupBy(\"track_id\").agg(\n",
    "        countDistinct(\"pid\").alias(\"intersection\")\n",
    "    )\n",
    "\n",
    "    #On crée un dernier dataset en joignant avec df_tracks_vectors pour récupérer la taille du vecteur B\n",
    "    recommendations_df = intersection_df.join(df_tracks_vectors, on=\"track_id\", how=\"inner\")\n",
    "\n",
    "    #Similarity score\n",
    "    result_df = recommendations_df.withColumn(\n",
    "        \"similarity_score\",\n",
    "        col(\"intersection\") / sqrt(lit(target_global_count) * col(\"pid_count\"))\n",
    "    )\n",
    "   \n",
    "    return result_df.select(\"track_id\", \"intersection\", \"pid_count\", \"similarity_score\") \\\n",
    "                    .orderBy(desc(\"similarity_score\")) \\\n",
    "                    .limit(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, size, array_intersect, arrays_overlap, sqrt, desc\n",
    "\n",
    "def similar_tracks(target_track_id, df_tracks_vectors, limit=10):\n",
    "    \n",
    "    # 1. Récupérer les infos de la cible (La liste des playlists et la taille)\n",
    "    # On récupère ça en Python localement (c'est très léger)\n",
    "    target_row = df_tracks_vectors.filter(col(\"track_id\") == target_track_id).first()\n",
    "    \n",
    "    if not target_row:\n",
    "        print(f\"Track {target_track_id} introuvable\")\n",
    "        return None\n",
    "        \n",
    "    target_pids_list = target_row['playlists_list'] # C'est une liste Python [1, 5, 8...]\n",
    "    target_global_count = target_row['pid_count']\n",
    "\n",
    "    # 2. Filtrer d'abord : On ne garde que les musiques qui ont au moins \n",
    "    # UNE playlist en commun avec la cible.\n",
    "    # arrays_overlap est très efficace pour ça.\n",
    "    candidates = df_tracks_vectors.filter(\n",
    "        (col(\"track_id\") != target_track_id) & # On s'exclut soi-même\n",
    "        arrays_overlap(col(\"playlists_list\"), lit(target_pids_list))\n",
    "    )\n",
    "\n",
    "    # 3. Calculer l'intersection directement dans la ligne\n",
    "    # On utilise array_intersect pour trouver les éléments communs, puis size() pour compter\n",
    "    result_df = candidates.withColumn(\n",
    "        \"intersection_count\", \n",
    "        size(array_intersect(col(\"playlists_list\"), lit(target_pids_list)))\n",
    "    )\n",
    "\n",
    "    # 4. Calculer le score (Même formule qu'avant)\n",
    "    final_df = result_df.withColumn(\n",
    "        \"similarity_score\",\n",
    "        col(\"intersection_count\") / sqrt(lit(target_global_count) * col(\"pid_count\"))\n",
    "    )\n",
    "\n",
    "    return final_df.select(\"track_id\", \"intersection_count\", \"pid_count\", \"similarity_score\") \\\n",
    "                   .orderBy(desc(\"similarity_score\")) \\\n",
    "                   .limit(limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3a097",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation de `similar_tracks`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced8c15",
   "metadata": {},
   "source": [
    "On a bien la track qui est similaire avec elle-même.\n",
    "\n",
    "**Notes importantes**: \n",
    "* ça m'a quand même pris 8 minutes pour run sur le dataset complet, ça reste long si on doit faire une démo live. Mais je calcule l'entérieté des similarité entre toutes les musiques.\n",
    "* J'ai essayé de comparer seulement une paire mais au final avoir le résultat prend plus de temps que de ressortir l'ensemble des comparaisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98257a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_track_id = \"35kahykNu00FPysz3C2euR\" \n",
    "\n",
    "recs = similar_tracks(sample_track_id, cleaned_df_track, df_tracks_vectors)\n",
    "\n",
    "if recs:\n",
    "    recs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd59180",
   "metadata": {},
   "source": [
    "# **TO DO** Product recommendation : Collaborative filtering with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780deef8",
   "metadata": {},
   "source": [
    "https://github.com/databricks/spark-training/blob/master/website/movie-recommendation-with-mllib.md\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-collaborative-filtering.html\n",
    "* solution 1 : ALS\n",
    "* solution 2: Word2Vec\n",
    "* Solution 3: sparse vecteur de Spark (?)\n",
    "\n",
    "Pouvoir expliquer pourquoi on a utilisé l'un et pas l'autre ???\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
